diff --git a/log_analyzer_project/log_analyzer/analyzer.py b/log_analyzer_project/log_analyzer/analyzer.py
index c50dd60..a1c8ef5 100644
--- a/log_analyzer_project/log_analyzer/analyzer.py
+++ b/log_analyzer_project/log_analyzer/analyzer.py
@@ -1,5 +1,16 @@
+from collections import Counter, defaultdict
+from typing import List
 
-from collections import Counter
+def top_ip_addresses(logs: List[dict], top_n: int = 10) -> dict:
+    """
+    Returns a dict of top N IPs and their hit counts.
+    """
+    counter = Counter()
+    for log in logs:
+        ip = log.get("ip")
+        if ip:
+            counter[ip] += 1
+    return dict(counter.most_common(top_n))
 
 def most_requested_endpoints(logs, top_n=5):
     endpoints = Counter(log["endpoint"] for log in logs)
@@ -9,3 +20,10 @@ def status_code_distribution(logs):
     codes = Counter(log["status"] for log in logs)
     return dict(codes)
 
+def traffic_per_hour(logs):
+    hourly = defaultdict(int)
+    for log in logs:
+        hour = log["timestamp"].strftime("%Y-%m-%d %H:00")
+        hourly[hour] += 1
+    return dict(hourly)
+
diff --git a/log_analyzer_project/log_analyzer/constants.py b/log_analyzer_project/log_analyzer/constants.py
new file mode 100644
index 0000000..156962c
--- /dev/null
+++ b/log_analyzer_project/log_analyzer/constants.py
@@ -0,0 +1,3 @@
+# Format: 10/Oct/2000:13:55:36 -0700
+APACHE_LOG_TIMESTAMP_FORMAT = "%d/%b/%Y:%H:%M:%S %z"
+
+
diff --git a/log_analyzer_project/log_analyzer/filters.py b/log_analyzer_project/log_analyzer/filters.py
new file mode 100644
index 0000000..9095898
--- /dev/null
+++ b/log_analyzer_project/log_analyzer/filters.py
@@ -0,0 +1,40 @@
+from typing import List, Optional
+from log_analyzer.utils import parse_timestamp
+
+def filter_by_time_range(
+    logs: List[dict], 
+    start: Optional[str] = None, 
+    end: Optional[str] = None
+) -> List[dict]:
+    """
+    Filters log entries between start and end datetime (inclusive).
+    Expects ISO 8601 datetime strings.
+    """
+    if not start and not end:
+        return logs
+
+    start_dt = parse_timestamp(start) if start else None
+    end_dt = parse_timestamp(end) if end else None
+
+    filtered = []
+    for entry in logs:
+        entry_time = entry.get("timestamp")
+        if not entry_time:
+            continue
+
+        if ((start_dt is None or entry_time >= start_dt) and
+            (end_dt is None or entry_time <= end_dt)):
+            filtered.append(entry)
+
+    return filtered
+
+
+def filter_error_logs(logs: List[dict]) -> List[dict]:
+    """
+    Returns only log entries with status codes 4xx or 5xx.
+    """
+    return [
+        log for log in logs 
+        if 400 <= int(log.get("status", 0)) < 600
+    ]
+
diff --git a/log_analyzer_project/log_analyzer/log_parser.py b/log_analyzer_project/log_analyzer/log_parser.py
index 3bcf09a..202b2b2 100644
--- a/log_analyzer_project/log_analyzer/log_parser.py
+++ b/log_analyzer_project/log_analyzer/log_parser.py
@@ -1,19 +1,14 @@
-import re
-from datetime import datetime
-
-LOG_PATTERN = re.compile(
-    r"(?P<ip>\S+) - - \[(?P<timestamp>[^\]]+)\] \"(?P<method>\S+) (?P<endpoint>\S+) \S+\" (?P<status>\d{3}) (?P<size>\d+)"
-)
-
 def parse_line(line):
     match = LOG_PATTERN.match(line)
     if not match:
         return None
     data = match.groupdict()
-    try:
-        data["timestamp"] = datetime.strptime(data["timestamp"], "%d/%b/%Y:%H:%M:%S %z")
-    except ValueError:
-        data["timestamp"] = None
+     # Parse with timezone info but then strip it to make it timezone-naiveAdd commentMore actions
+    
+    dt = datetime.strptime(data["timestamp"], APACHE_LOG_TIMESTAMP_FORMAT)
+    data["timestamp"] = dt.replace(tzinfo=None)
+
+
     return data
 
 def read_log_file(filepath):
diff --git a/log_analyzer_project/log_analyzer/main.py b/log_analyzer_project/log_analyzer/main.py
index b152727..5af93e8 100644
--- a/log_analyzer_project/log_analyzer/main.py
+++ b/log_analyzer_project/log_analyzer/main.py
@@ -1,27 +1,75 @@
-import sys
+
+import argparse
 from log_analyzer.log_parser import read_log_file
-from log_analyzer.analyzer import most_requested_endpoints, status_code_distribution
-from log_analyzer.report import print_endpoint_stats, print_status_codes
+from log_analyzer.analyzer import (
+    most_requested_endpoints, 
+    status_code_distribution,
+    traffic_per_hour,
+    top_ip_addresses
+)
+from log_analyzer.report import (
+    print_endpoint_stats, 
+    print_status_codes, 
+    print_traffic,
+    print_ip_stats
+)
+from log_analyzer.filters import (
+    filter_by_time_range, 
+    filter_error_logs
+)
+from log_analyzer.utils import parse_timestamp
+import sys
 
 def main():
-    import sys
-    if len(sys.argv) != 2:
-        print("Usage: log-analyzer <access_log_file>")
+    parser = argparse.ArgumentParser(description="Analyze Apache-style log files.")
+    parser.add_argument("logfile", help="Path to the access log file")
+    parser.add_argument("--top", type=int, default=10, help="Top N entries to display")
+    parser.add_argument("--start", type=str, help="Start datetime in ISO format (e.g. 2025-06-04T06:00:00)")
+    parser.add_argument("--end", type=str, help="End datetime in ISO format (e.g. 2025-06-04T12:00:00)")
+    parser.add_argument("--errors-only", action="store_true", help="Include only 4xx and 5xx error logs")
+
+    args = parser.parse_args()
+    parse_timestamp(args.start)
+    parse_timestamp(args.end)
+
+    try:
+        logs = list(read_log_file(args.logfile))
+    except FileNotFoundError:
+        print(f"Log file not found: {args.logfile}")
         sys.exit(1)
 
-    log_file_path = sys.argv[1]
-    logs = list(read_log_file(log_file_path))
+    if args.start or args.end:
+        logs = filter_by_time_range(logs, args.start, args.end)
+
+    if args.errors_only:
+        logs = filter_error_logs(logs)
 
-    endpoint_stats = most_requested_endpoints(logs)
-    status_codes = status_code_distribution(logs)
+    # Analysis
+    endpoint_stats = most_requested_endpoints(logs, top_n=args.top)
+    code_stats = status_code_distribution(logs)
+    hourly_traffic = traffic_per_hour(logs)
+    ip_stats = top_ip_addresses(logs, top_n=args.top)
 
+    # Reporting
     print_endpoint_stats(endpoint_stats)
-    print_status_codes(status_codes)
+    print_status_codes(code_stats)
+    print_traffic(hourly_traffic)
+    print_ip_stats(ip_stats)
 
 if __name__ == "__main__":
-    if len(sys.argv) != 2:
-        print("Usage: python main.py access.log")
-        sys.exit(1)
-    main(sys.argv[1])
+    main()
 
diff --git a/log_analyzer_project/log_analyzer/report.py b/log_analyzer_project/log_analyzer/report.py
index f658ed7..7d8782d 100644
--- a/log_analyzer_project/log_analyzer/report.py
+++ b/log_analyzer_project/log_analyzer/report.py
@@ -8,3 +8,13 @@ def print_status_codes(stats):
     for code, count in stats.items():
         print(f"{code}: {count}")
 
+def print_traffic(stats):
+    print("\nTraffic per Hour:")
+    for hour, count in sorted(stats.items()):
+        print(f"{hour}: {count} requests")
+
+def print_ip_stats(ip_stats: dict) -> None:
+    print("\nTop IP Addresses:")
+    for ip, count in ip_stats.items():
+        print(f"  {ip:<15} -> {count} hits")
+
diff --git a/log_analyzer_project/log_analyzer/utils.py b/log_analyzer_project/log_analyzer/utils.py
new file mode 100644
index 0000000..f0d88d5
--- /dev/null
+++ b/log_analyzer_project/log_analyzer/utils.py
@@ -0,0 +1,27 @@
+from datetime import datetime
+from typing import Optional
+
+from log_analyzer.constants import APACHE_LOG_TIMESTAMP_FORMAT
+
+def parse_timestamp(dt_str: Optional[str]) -> Optional[datetime]:
+    if dt_str is None:
+        return None
+    try:
+        # Parse the datetime and ensure it's timezone-naive
+        dt = datetime.fromisoformat(dt_str)
+        if dt.tzinfo is not None:
+            dt = dt.replace(tzinfo=None)
+        return dt
+    except ValueError as e:
+        raise ValueError(f"Invalid datetime format: '{dt_str}'. Use ISO format like '2025-06-04T06:00:00'.") from e
+
+def parse_apache_timestamp(ts: str) -> Optional[datetime]:
+    """
+    Parses Apache-style timestamp and removes timezone info to make it offset-naive.
+    """
+    try:
+        dt = datetime.strptime(ts, APACHE_LOG_TIMESTAMP_FORMAT)
+        return dt.replace(tzinfo=None)  # Strip timezone
+    except ValueError:
+        return None
+
